{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-26T18:37:15.744797Z",
     "start_time": "2024-05-26T18:37:11.639196Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T18:37:18.586377Z",
     "start_time": "2024-05-26T18:37:18.573853Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create a small dataset\n",
    "data = {\n",
    "    \"sentence\": [\n",
    "        \"Braulio Jose Cespedes Acosta\",\n",
    "        \"Toronto, ON\",\n",
    "        \"(437) 733-7952\",\n",
    "        \"brauliojose.cespedesacosta@georgebrown.ca\",\n",
    "        \"Java\",\n",
    "        \"Python\",\n",
    "        \"Oracle\",\n",
    "        \"MS SQL server\",\n",
    "        \"MySQL\",\n",
    "        \"Git\",\n",
    "        \"Microsoft Office\",\n",
    "        \"Java Mockito\",\n",
    "        \"Python Pytest\",\n",
    "        \"Tableau\",\n",
    "        \"Hadoop\",\n",
    "        \"Spark\",\n",
    "        \"Hive\",\n",
    "        \"Apache Pig\"\n",
    "    ],\n",
    "    \"label\": [\n",
    "        \"O O O B-NAME\",\n",
    "        \"O O\",\n",
    "        \"O O O O O O\",\n",
    "        \"O O O O O\",\n",
    "        \"B-SKILL\",\n",
    "        \"B-SKILL\",\n",
    "        \"B-SKILL\",\n",
    "        \"B-SKILL I-SKILL\",\n",
    "        \"B-SKILL\",\n",
    "        \"B-SKILL\",\n",
    "        \"B-SKILL I-SKILL\",\n",
    "        \"B-SKILL I-SKILL\",\n",
    "        \"B-SKILL I-SKILL\",\n",
    "        \"B-SKILL\",\n",
    "        \"B-SKILL\",\n",
    "        \"B-SKILL\",\n",
    "        \"B-SKILL\",\n",
    "        \"B-SKILL I-SKILL\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)"
   ],
   "id": "d015a3afc3ee7ca7",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T18:37:19.513239Z",
     "start_time": "2024-05-26T18:37:19.483137Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Convert to Hugging Face Dataset\n",
    "dataset = Dataset.from_pandas(df)"
   ],
   "id": "d5cc67910f128a98",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T18:37:20.239954Z",
     "start_time": "2024-05-26T18:37:20.220406Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i, label in enumerate(dataset[\"label\"]):\n",
    "    print(f\"i:{i}, label:{label}\")"
   ],
   "id": "fe62fa2f03736ba5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i:0, label:O O O B-NAME\n",
      "i:1, label:O O\n",
      "i:2, label:O O O O O O\n",
      "i:3, label:O O O O O\n",
      "i:4, label:B-SKILL\n",
      "i:5, label:B-SKILL\n",
      "i:6, label:B-SKILL\n",
      "i:7, label:B-SKILL I-SKILL\n",
      "i:8, label:B-SKILL\n",
      "i:9, label:B-SKILL\n",
      "i:10, label:B-SKILL I-SKILL\n",
      "i:11, label:B-SKILL I-SKILL\n",
      "i:12, label:B-SKILL I-SKILL\n",
      "i:13, label:B-SKILL\n",
      "i:14, label:B-SKILL\n",
      "i:15, label:B-SKILL\n",
      "i:16, label:B-SKILL\n",
      "i:17, label:B-SKILL I-SKILL\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T18:37:21.708064Z",
     "start_time": "2024-05-26T18:37:21.435728Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokenized_inputs = None\n",
    "tokenized_inputs = tokenizer(dataset[\"label\"], truncation=True, padding=True, is_split_into_words=True)"
   ],
   "id": "496e0007490b3893",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T18:41:55.225627Z",
     "start_time": "2024-05-26T18:41:55.211509Z"
    }
   },
   "cell_type": "code",
   "source": "tokenized_inputs.word_ids(batch_index=0)",
   "id": "eb1640616537ba66",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " None]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T18:30:47.266311Z",
     "start_time": "2024-05-26T18:30:39.921623Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenize the sentences\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"sentence\"], truncation=True, padding=True, is_split_into_words=True)\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"label\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label.split()[word_idx])\n",
    "            else:\n",
    "                label_ids.append(label.split()[word_idx] if label.split()[word_idx].startswith(\"I-\") else -100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Tokenize dataset\n",
    "tokenized_dataset = dataset.map(tokenize_and_align_labels, batched=True)"
   ],
   "id": "52f3e11501772b94",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/18 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fc5a3aba9c8d4acba6f0493365d1be59"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 25\u001B[0m\n\u001B[0;32m     22\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m tokenized_inputs\n\u001B[0;32m     24\u001B[0m \u001B[38;5;66;03m# Tokenize dataset\u001B[39;00m\n\u001B[1;32m---> 25\u001B[0m tokenized_dataset \u001B[38;5;241m=\u001B[39m \u001B[43mdataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtokenize_and_align_labels\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatched\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Developments\\Python\\DeepLearning\\NER_Resume\\.venv\\lib\\site-packages\\datasets\\arrow_dataset.py:602\u001B[0m, in \u001B[0;36mtransmit_tasks.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    600\u001B[0m     \u001B[38;5;28mself\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m=\u001B[39m kwargs\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mself\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    601\u001B[0m \u001B[38;5;66;03m# apply actual function\u001B[39;00m\n\u001B[1;32m--> 602\u001B[0m out: Union[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDatasetDict\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    603\u001B[0m datasets: List[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(out\u001B[38;5;241m.\u001B[39mvalues()) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(out, \u001B[38;5;28mdict\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m [out]\n\u001B[0;32m    604\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m dataset \u001B[38;5;129;01min\u001B[39;00m datasets:\n\u001B[0;32m    605\u001B[0m     \u001B[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001B[39;00m\n",
      "File \u001B[1;32mD:\\Developments\\Python\\DeepLearning\\NER_Resume\\.venv\\lib\\site-packages\\datasets\\arrow_dataset.py:567\u001B[0m, in \u001B[0;36mtransmit_format.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    560\u001B[0m self_format \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m    561\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtype\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_type,\n\u001B[0;32m    562\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mformat_kwargs\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_kwargs,\n\u001B[0;32m    563\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcolumns\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_columns,\n\u001B[0;32m    564\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moutput_all_columns\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_output_all_columns,\n\u001B[0;32m    565\u001B[0m }\n\u001B[0;32m    566\u001B[0m \u001B[38;5;66;03m# apply actual function\u001B[39;00m\n\u001B[1;32m--> 567\u001B[0m out: Union[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDatasetDict\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    568\u001B[0m datasets: List[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(out\u001B[38;5;241m.\u001B[39mvalues()) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(out, \u001B[38;5;28mdict\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m [out]\n\u001B[0;32m    569\u001B[0m \u001B[38;5;66;03m# re-apply format to the output\u001B[39;00m\n",
      "File \u001B[1;32mD:\\Developments\\Python\\DeepLearning\\NER_Resume\\.venv\\lib\\site-packages\\datasets\\arrow_dataset.py:3156\u001B[0m, in \u001B[0;36mDataset.map\u001B[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001B[0m\n\u001B[0;32m   3150\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m transformed_dataset \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   3151\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m hf_tqdm(\n\u001B[0;32m   3152\u001B[0m         unit\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m examples\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   3153\u001B[0m         total\u001B[38;5;241m=\u001B[39mpbar_total,\n\u001B[0;32m   3154\u001B[0m         desc\u001B[38;5;241m=\u001B[39mdesc \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMap\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   3155\u001B[0m     ) \u001B[38;5;28;01mas\u001B[39;00m pbar:\n\u001B[1;32m-> 3156\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m rank, done, content \u001B[38;5;129;01min\u001B[39;00m Dataset\u001B[38;5;241m.\u001B[39m_map_single(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mdataset_kwargs):\n\u001B[0;32m   3157\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m done:\n\u001B[0;32m   3158\u001B[0m                 shards_done \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[1;32mD:\\Developments\\Python\\DeepLearning\\NER_Resume\\.venv\\lib\\site-packages\\datasets\\arrow_dataset.py:3547\u001B[0m, in \u001B[0;36mDataset._map_single\u001B[1;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001B[0m\n\u001B[0;32m   3543\u001B[0m indices \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\n\u001B[0;32m   3544\u001B[0m     \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m*\u001B[39m(\u001B[38;5;28mslice\u001B[39m(i, i \u001B[38;5;241m+\u001B[39m batch_size)\u001B[38;5;241m.\u001B[39mindices(shard\u001B[38;5;241m.\u001B[39mnum_rows)))\n\u001B[0;32m   3545\u001B[0m )  \u001B[38;5;66;03m# Something simpler?\u001B[39;00m\n\u001B[0;32m   3546\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 3547\u001B[0m     batch \u001B[38;5;241m=\u001B[39m \u001B[43mapply_function_on_filtered_inputs\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   3548\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3549\u001B[0m \u001B[43m        \u001B[49m\u001B[43mindices\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3550\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcheck_same_num_examples\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mshard\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlist_indexes\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m>\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3551\u001B[0m \u001B[43m        \u001B[49m\u001B[43moffset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moffset\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3552\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3553\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m NumExamplesMismatchError:\n\u001B[0;32m   3554\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m DatasetTransformationNotAllowedError(\n\u001B[0;32m   3555\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUsing `.map` in batched mode on a dataset with attached indexes is allowed only if it doesn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt create or remove existing examples. You can first run `.drop_index() to remove your index and then re-add it.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   3556\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32mD:\\Developments\\Python\\DeepLearning\\NER_Resume\\.venv\\lib\\site-packages\\datasets\\arrow_dataset.py:3416\u001B[0m, in \u001B[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001B[1;34m(pa_inputs, indices, check_same_num_examples, offset)\u001B[0m\n\u001B[0;32m   3414\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m with_rank:\n\u001B[0;32m   3415\u001B[0m     additional_args \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m (rank,)\n\u001B[1;32m-> 3416\u001B[0m processed_inputs \u001B[38;5;241m=\u001B[39m function(\u001B[38;5;241m*\u001B[39mfn_args, \u001B[38;5;241m*\u001B[39madditional_args, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfn_kwargs)\n\u001B[0;32m   3417\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(processed_inputs, LazyDict):\n\u001B[0;32m   3418\u001B[0m     processed_inputs \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m   3419\u001B[0m         k: v \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m processed_inputs\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mitems() \u001B[38;5;28;01mif\u001B[39;00m k \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m processed_inputs\u001B[38;5;241m.\u001B[39mkeys_to_format\n\u001B[0;32m   3420\u001B[0m     }\n",
      "Cell \u001B[1;32mIn[5], line 16\u001B[0m, in \u001B[0;36mtokenize_and_align_labels\u001B[1;34m(examples)\u001B[0m\n\u001B[0;32m     14\u001B[0m     label_ids\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m100\u001B[39m)\n\u001B[0;32m     15\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m word_idx \u001B[38;5;241m!=\u001B[39m previous_word_idx:\n\u001B[1;32m---> 16\u001B[0m     label_ids\u001B[38;5;241m.\u001B[39mappend(\u001B[43mlabel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msplit\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m[\u001B[49m\u001B[43mword_idx\u001B[49m\u001B[43m]\u001B[49m)\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     18\u001B[0m     label_ids\u001B[38;5;241m.\u001B[39mappend(label\u001B[38;5;241m.\u001B[39msplit()[word_idx] \u001B[38;5;28;01mif\u001B[39;00m label\u001B[38;5;241m.\u001B[39msplit()[word_idx]\u001B[38;5;241m.\u001B[39mstartswith(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mI-\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m100\u001B[39m)\n",
      "\u001B[1;31mIndexError\u001B[0m: list index out of range"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T18:29:19.286061Z",
     "start_time": "2024-05-26T18:29:19.284818Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize the model\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "# Set training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    eval_dataset=tokenized_dataset,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ],
   "id": "d541763c589922e2",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
